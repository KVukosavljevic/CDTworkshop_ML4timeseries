{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e227758",
   "metadata": {},
   "source": [
    "# Dynamic risk scores practical session\n",
    "\n",
    "In this lab you'll practice:\n",
    "* (static) survival analysis\n",
    "  * basic data preprocessing: 1 hot encoding, mean imputation, normalisation.\n",
    "  * boosting model: CoxBoost\n",
    "  * survival random forests\n",
    "* dynamic survival analysis\n",
    "  * basic data processing\n",
    "  * 2 stage model\n",
    "  * CNN\n",
    "\n",
    "\n",
    "# Prepare the environment\n",
    "\n",
    "install the necessary packages in the current environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561857ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib lifelines sklearn scikit-survival statsmodels tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6fef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21a634",
   "metadata": {},
   "source": [
    "# (Static) Risk score\n",
    "\n",
    "In the first part of our lab we'll use an open source dataset to evaluate some of the models used for Survival Analysis. We will use Concordance Index on test to score them. We will prepare the data, split the data in train and test, tune the hyperparameters using 5 fold cross validation on the train set, and evaluate each model on the test set. \n",
    "\n",
    "## The dataset\n",
    "\n",
    "We'll use the lung cancer dataset (from the R `survival` package, see https://stat.ethz.ch/R-manual/R-devel/library/survival/html/lung.html)\n",
    "\n",
    "* `inst`: Institution code\n",
    "* `time`: Survival time in days\n",
    "* `status`: censoring status 1=censored, 2=dead\n",
    "* `age`: Age in years\n",
    "* `sex`: Male=1 Female=2\n",
    "* `ph.ecog`: ECOG performance score (0=good 5=dead)\n",
    "* `ph.karno`: Karnofsky performance score as rated by physician\n",
    "* `pat.karno`: Karnofsky performance score as rated by patient\n",
    "* `meal.cal`: Calories consumed at meals\n",
    "* `wt.loss`: Weight loss in last six months\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8115ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = pd.read_csv(\"data/cancer.csv\", index_col=0)\n",
    "cancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = [\"inst\", \"ph.ecog\"]\n",
    "continuous_variables = set(cancer.columns) - set([\"time\", \"status\", \"sex\"]) - set(categorical_variables)\n",
    "\n",
    "print(f\"the continuous variables are {continuous_variables}\")\n",
    "print(f\"the categorical variables are {categorical_variables}\")\n",
    "\n",
    "print(f\"the inst column only has {len(cancer.inst.unique())} unique values\")\n",
    "inst_freqs = cancer.groupby('inst').size()\n",
    "print(f\"{np.sum(inst_freqs < 10)} values have less than 10 instances in the dataset, we'll make a special institution for them\")\n",
    "print(f\"we will make a special institution for {list(inst_freqs.index[inst_freqs < 10])}, together with missing values\")\n",
    "print(\"institution will also be 1 hot encoded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b31ba03",
   "metadata": {},
   "source": [
    "The `inst` column tells us from which hospital this subjects comes, it might have an impact on survival probability, so we keep this column. However, as shown above for some institutions we don't have enough data, and this will likely lead to problems. One possible approach is to remove the institutions that don't have enough subjects in this dataset. They will all be handled as if they were the same institution. This can be done quickly by simply assigning NaN to those subjects (we will handle NaNs later in this script). \n",
    "\n",
    "Now find all the rows with `inst` equal to a value that has less than 10 instances in the dataset and set `inst` to NaN for that rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fafc82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set `cancer.inst` to np.nan if that institution has less than 10 samples in this dataset\n",
    "# in other words if is in this list list(inst_freqs.index[inst_freqs < 10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc3564d",
   "metadata": {},
   "source": [
    "## 1 hot encoding \n",
    "\n",
    "Let's now 1 hot encode the columns that are in the `categorical_variables` variable, you can use `sklearn.preprocessing.OneHotEncoder` for this task.\n",
    "\n",
    "To avoid colinearity make sure you drop one of the output columns (e.g. using `drop=\"first\"` in `OneHotEncoder`).\n",
    "\n",
    "After this step the dataset should look similar to this (not necessarily with the same column names):\n",
    "\n",
    "| time | status | age | sex | ph.karno | pat.karno | meal.cal | wt.loss | inst_3.0 | inst_6.0 | ... | inst_12.0 | inst_13.0 | inst_16.0 | inst_21.0 | inst_22.0 | inst_nan | ph.ecog_1.0 | ph.ecog_2.0 | ph.ecog_3.0 | ph.ecog_nan |     |\n",
    "|-----:|-------:|----:|----:|---------:|----------:|---------:|--------:|---------:|---------:|----:|----------:|----------:|----------:|----------:|----------:|---------:|------------:|------------:|------------:|------------:|-----|\n",
    "|    1 |    306 |   2 |  74 |        1 |      90.0 |    100.0 |  1175.0 |      NaN |      1.0 | 0.0 |       ... |       0.0 |       0.0 |       0.0 |       0.0 |      0.0 |         0.0 |         1.0 |         0.0 |         0.0 | 0.0 |\n",
    "|    2 |    455 |   2 |  68 |        1 |      90.0 |     90.0 |  1225.0 |     15.0 |      1.0 | 0.0 |       ... |       0.0 |       0.0 |       0.0 |       0.0 |      0.0 |         0.0 |         0.0 |         0.0 |         0.0 | 0.0 |\n",
    "|    3 |   1010 |   1 |  56 |        1 |      90.0 |     90.0 |     NaN |     15.0 |      1.0 | 0.0 |       ... |       0.0 |       0.0 |       0.0 |       0.0 |      0.0 |         0.0 |         0.0 |         0.0 |         0.0 | 0.0 |\n",
    "|    4 |    210 |   2 |  57 |        1 |      90.0 |     60.0 |  1150.0 |     11.0 |      0.0 | 0.0 |       ... |       0.0 |       0.0 |       0.0 |       0.0 |      0.0 |         1.0 |         0.0 |         0.0 |         0.0 | 1.0 |\n",
    "|    5 |    883 |   2 |  60 |        1 |     100.0 |     90.0 |     NaN |      0.0 |      0.0 | 0.0 |       ... |       0.0 |       0.0 |       0.0 |       0.0 |      0.0 |         0.0 |         0.0 |         0.0 |         0.0 | 0.0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831219b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply one hot encoding to the variables in the categorical_variables list\n",
    "# drop one of the generated columns to avoid collinearity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28b5768",
   "metadata": {},
   "source": [
    "Let's see how many subjects we have in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c657b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(cancer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82adddf8",
   "metadata": {},
   "source": [
    "we don't have enough subjects in `ph.ecog_3.0` and `ph.ecog_nan`. We'll drop them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac63396",
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = cancer.drop(columns=['ph.ecog_3.0', 'ph.ecog_nan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3a256",
   "metadata": {},
   "source": [
    "## Splitting the data in test / train\n",
    "\n",
    "Before performing mean imputation and normalisation let's split the data in test and train (we should have done that before removing institutions with less than 10 subjects, but it would have complicated the script, so for sake of brevity we will pretend this didn't happen!).\n",
    "\n",
    "Now split `cancer` in two dataframes: `train_data` and `test_data`, stratified on the columns `status` of the `cancer` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa7157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset in `train_data` and `test_data`, split by patient (patient-wise).\n",
    "\n",
    "# your solution here\n",
    "\n",
    "print(f\"the train dataset has shape {train_data.shape}, the test dataset has shape {test_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a1c6f5",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "\n",
    "Let's not impute missing values using mean imputation (different imputation strategies should be evaluated, probably using cross validation in the training set. Again, in the interest of time we will only look at mean imputation). \n",
    "\n",
    "Train a `sklearn.impute.SimpleImputer` using `mean` strategy on the training set, and transform both the training and the test sets.\n",
    "\n",
    "Apply mean imputation to all the columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f1bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply mean imputation to remove all NaNs\n",
    "\n",
    "# your solution here\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0033aeb",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "\n",
    "Let's now normalise each column. This will let us see the impact of each predictor in CoxPH models, and help with the fitting of the ML models.\n",
    "\n",
    "Like before, train a `sklearn.preprocessing.Normalizer` on the trainig data only, and transform the training and test datasets. \n",
    "\n",
    "Apply normalisation to the columns in the `continuous_variables` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f48999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise the dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9df269",
   "metadata": {},
   "source": [
    "## Cox PH model\n",
    "\n",
    "Let's now fit a CoxPH model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f035a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(train_data, duration_col='time', event_col='status')\n",
    "\n",
    "cph.print_summary()\n",
    "cph.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3725397a",
   "metadata": {},
   "source": [
    "Let's see which of the covariates are significant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ace49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_covariates = cph.summary\n",
    "cox_covariates[cox_covariates.p < 0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc75b4",
   "metadata": {},
   "source": [
    "We can see that `ph.karno` is a strong risk factor (high values increase ther risk), the same can be said for `ph.ecog`. Belonging to institution number 16 seems to be a strong protective factor.\n",
    "\n",
    "Let's see how this model performs on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1e7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_validation_score = cph.score(test_data, scoring_method=\"concordance_index\")\n",
    "print(f\"CoxPH model has CI={cox_validation_score} in test, with CI={cph.concordance_index_} in training\")\n",
    "\n",
    "# we will append CI on test set for all models to this array\n",
    "results = []\n",
    "\n",
    "results.append((\"CoxPH\", cox_validation_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f429d",
   "metadata": {},
   "source": [
    "## CoxBoost\n",
    "\n",
    "Let's now try using a boosting algorithm. We can use XGBoost or CoxBoost. For this exercise let's use CoxBoost, that is available in `scikit-survival`.\n",
    "\n",
    "The models in `sksurv` expect `y` to be an array of tuples of the form `(Boolean, flaot)` where the first value is `True` if that subject experienced the event, `False` if censored, and the second value is the time to event (or to censoring).\n",
    "In our dataset the event is present if `data.status == 2` where `data` can be `train_data` or `test_data`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08173fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data_for_sksurv(data):\n",
    "    X = data.loc[:, set(data.columns) - set([\"time\", \"status\"])]\n",
    "    Y = pd.DataFrame({\n",
    "        'event': data.status == 2,\n",
    "        'time': data.time\n",
    "        }).to_numpy()\n",
    "    Y=np.array([tuple(row) for row in Y], dtype=[('event', '?'), ('time', '<f8')])\n",
    "    return X, Y\n",
    "\n",
    "X_train, y_train = prepare_data_for_sksurv(train_data)\n",
    "X_test, y_test = prepare_data_for_sksurv(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda094ad",
   "metadata": {},
   "source": [
    "Now use `sksurv.ensemble.GradientBoostingSurvivalAnalysis` to train and evaluate a CoxBoost model.\n",
    "\n",
    "Use the `coxph` loss.\n",
    "\n",
    "Perform a grid search for the parameters.\n",
    "\n",
    "Use 5-fold-cross-validation on the trainng set to perform the hyperparameter search. \n",
    "Then train the best model on the whole training set, and evaluate its performance on the test set.\n",
    "\n",
    "`sklearn.model_selection.GridSearchCV` will do all the heavy lifting for you!\n",
    "\n",
    "The choice of which parameters to explore is up to you (be mindful that it might take a lot of time to perform a grid search over a large space). These are the paramters I used:\n",
    "\n",
    "```\n",
    "parameters = {\n",
    "    'learning_rate':[0.001, 0.01, 0.05, 0.1, 0.25, 0.5],\n",
    "    'n_estimators':[10, 50, 100, 250],\n",
    "    'max_depth':[2,3,5]\n",
    "        }\n",
    "```\n",
    "\n",
    "Append the CI on the test set to the `results` list like we did for the CoxPH model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755cd242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744cd26b",
   "metadata": {},
   "source": [
    "## Survival Random Forests\n",
    "\n",
    "Now repeat the same steps for Survival Random Forests, use `sksurv.ensemble.RandomSurvivalForest`. \n",
    "The code is pratically the same!\n",
    "\n",
    "The choice of which parameters to explore is up to you (be mindful that it might take a lot of time to perform a grid search over a large space). These are the paramters I used:\n",
    "\n",
    "```\n",
    "parameters = {\n",
    "    'min_samples_split':[2, 6, 12, 24, 48],\n",
    "    'n_estimators':[10, 50, 100, 250],\n",
    "    'max_depth':[2,4, 8, 16]\n",
    "        }\n",
    "```\n",
    "\n",
    "Append the CI on the test set to the `results` list like we did for the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7bdc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65862a53",
   "metadata": {},
   "source": [
    "## Survival Suppoer Vector Machines\n",
    "\n",
    "Let's now try Linear Survival SVM, use the `sksurv.svm.FastSurvivalSVM` class.\n",
    "Set a value for `max_iter` higher than the default (I've set 100).\n",
    "Like before use `GridSearchCV` to find the best value for `alpha`. \n",
    "I've set this for the parameters:\n",
    "```\n",
    "parameters = {'alpha': 2. ** np.arange(-4, 4, 1)}\n",
    "```\n",
    "\n",
    "Append the CI on the test set to the `results` list like we did for the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e95b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf59a77",
   "metadata": {},
   "source": [
    "We have seen CoxPH, CoxBoost, Survival Random Forests and Survival SMVs. \n",
    "We have split the data in train test and used 5 fold cross validation to tune the hyperparameters.\n",
    "In our task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b09bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results, columns = [\"Model\", \"CI\"]).sort_values(\"CI\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deefe67f",
   "metadata": {},
   "source": [
    "# Dynamic risk score\n",
    "\n",
    "Now let's have a look at how to bring \"time series\" and \"survival analysis\" together.\n",
    "\n",
    "## The dataset\n",
    "\n",
    "Let's load the longitudinal dataset and explore the longitudinal and survival variables.\n",
    "\n",
    "We use the `pbc2` dataset. This data is from the Mayo Clinic trial in primary biliary cirrhosis (PBC) of the liver conducted between 1974 and 1984. A total of 424 PBC patients, referred to Mayo Clinic during that ten-year interval met eligibility criteria for the randomized placebo controlled trial of the drug D-penicillamine, but only the first 312 cases in the data set participated in the randomized trial. Therefore, the data here are for the 312 patients with largely complete data.\n",
    "\n",
    "See https://rdrr.io/cran/joineRML/man/pbc2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea05e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbc2 = pd.read_csv(\"data/pbc2.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeffdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbc2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"we have {len(pbc2.id.unique())} subjects, {len(pbc2)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69848f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove subjects with less than 3 entries\n",
    "good_ids = pbc2.id.unique()[[len(pbc2.loc[pbc2.id == x, :])>=3 for x in pbc2.id.unique()]]\n",
    "pbc2 = pbc2.loc[pbc2.id.isin(good_ids), :]\n",
    "\n",
    "print(f\"after removing subjects with less than 5 entries we have {len(pbc2.id.unique())} subjects, {len(pbc2)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faac1c1",
   "metadata": {},
   "source": [
    "As we did before, we split the dataset in train and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d261bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pbc2_survival = pbc2.loc[:, [\"id\", \"status\"]]\n",
    "pbc2_survival = pbc2_survival.drop_duplicates()\n",
    "\n",
    "train_ids, test_ids = train_test_split(\n",
    "    pbc2_survival.id, \n",
    "    test_size = 0.2, \n",
    "    random_state = 42, \n",
    "    stratify=pbc2_survival.status)\n",
    "train_data = pbc2.loc[pbc2.id.isin(train_ids), :]\n",
    "test_data = pbc2.loc[pbc2.id.isin(test_ids), :]\n",
    "\n",
    "# delete pbc2_survival, we'll re-instantiate later\n",
    "del pbc2_survival\n",
    "print(f\"the train dataset has shape {train_data.shape}, the test dataset has shape {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4052084",
   "metadata": {},
   "source": [
    "## Longitudinal variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a2f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "longitudinal_columns = list(train_data.columns[-9:-2])\n",
    "columns_to_normalise = longitudinal_columns + ['age']\n",
    "\n",
    "# like before, only use the training set to normalise\n",
    "normalisation_means = np.nanmean(train_data.loc[:, columns_to_normalise], axis=0)\n",
    "normalisation_sd = np.nanstd(train_data.loc[:, columns_to_normalise], axis=0)\n",
    "\n",
    "print(f\"\"\"normalisation mean and std for columns {columns_to_normalise} are \n",
    "{normalisation_means} \n",
    "and \n",
    "{normalisation_sd}\"\"\")\n",
    "\n",
    "normalised_columns = train_data.loc[:, columns_to_normalise]\n",
    "normalised_columns = (normalised_columns - normalisation_means) / normalisation_sd\n",
    "train_data.loc[:, columns_to_normalise] = normalised_columns\n",
    "\n",
    "# also normalise the test set\n",
    "test_data.loc[:, columns_to_normalise] = (test_data.loc[:, columns_to_normalise] - normalisation_means) / normalisation_sd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c605446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_limit = 50\n",
    "\n",
    "def plot_longitudinal(longitudinal_var=\"serBilir\"):\n",
    "    for my_id in train_data.id.unique()[:n_limit]:\n",
    "        my_data = train_data[train_data.id == my_id]\n",
    "        plt.plot(my_data.year, my_data[longitudinal_var])\n",
    "\n",
    "    plt.xlabel(\"years\")\n",
    "    plt.ylabel(longitudinal_var)\n",
    "    plt.title(longitudinal_var)\n",
    "    plt.show()\n",
    "\n",
    "for x in longitudinal_columns:\n",
    "    plot_longitudinal(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626ef329",
   "metadata": {},
   "source": [
    "## Survival variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba94e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_survival = train_data.loc[:, [\"id\", \"years\", \"status\", \"drug\", \"age\", \"sex\", \"ascites\", \"hepatomegaly\", \"spiders\", \"edema\"]]\n",
    "train_data_survival.head()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596b3968",
   "metadata": {},
   "source": [
    "We have the following variables:\n",
    "* `years`: the survival time\n",
    "* `status`: the censoring status: our event is \"dead\", \"alive\" and \"transplanted\" are censored\n",
    "* `drug`, `age`, `sex`, `ascitres`, `hepatomegaly`, `spiders`, `edema`: covariates, patient characteristic that don't change over time\n",
    "\n",
    "`age` is continuous, the other covariates are either binary or categorical. Let's see which one are binary and which one will require 1 hot encoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0366bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_values_of_variable(var_name):\n",
    "    print(f\"{var_name} has the following values {train_data_survival[var_name].unique()}\")\n",
    "\n",
    "vars = set(train_data_survival.columns[3:]) - {'age'}\n",
    "for x in vars:\n",
    "    print_values_of_variable(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6152ba6",
   "metadata": {},
   "source": [
    "\n",
    "* `edema` is categorical, with no NaNs. We will 1 hot encode this variable.\n",
    "* `sex` and `drig` are binary, with no NaNs, we will turn these variables to 0s and 1s\n",
    "* `spiders`, `hepatomegaly`, and `ascites` are binary with NaNs.  We will 1 hot encode this variable (so we don't drop subjects, being the dataset already small)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423fbc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_1_hot_encoded = ['edema', 'spiders', 'hepatomegaly', 'ascites']\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881630ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder.fit(train_data_survival.loc[:,to_be_1_hot_encoded])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_survival_one_hot_encoded = pd.DataFrame(\n",
    "    one_hot_encoder.transform(train_data_survival.loc[:,to_be_1_hot_encoded]).toarray(),\n",
    "    columns=one_hot_encoder.get_feature_names(to_be_1_hot_encoded),\n",
    "    index = train_data_survival.index)\n",
    "\n",
    "train_data = pd.concat([train_data, train_data_survival_one_hot_encoded], axis=1)\n",
    "train_data = train_data.drop(columns=to_be_1_hot_encoded)\n",
    "train_data.drug = train_data.drug.apply(lambda x: 1 if x == \"D-penicil\" else 0)\n",
    "train_data.sex = train_data.sex.apply(lambda x: 1 if x == \"female\" else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8644c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_survival = test_data.loc[:, [\"id\", \"years\", \"status\", \"drug\", \"age\", \"sex\", \"ascites\", \"hepatomegaly\", \"spiders\", \"edema\"]]\n",
    "test_data_survival_one_hot_encoded = pd.DataFrame(\n",
    "    one_hot_encoder.transform(test_data_survival.loc[:,to_be_1_hot_encoded]).toarray(),\n",
    "    columns=one_hot_encoder.get_feature_names(to_be_1_hot_encoded),\n",
    "    index = test_data_survival.index)\n",
    "\n",
    "test_data = pd.concat([test_data, test_data_survival_one_hot_encoded], axis=1)\n",
    "test_data = test_data.drop(columns=to_be_1_hot_encoded)\n",
    "test_data.drug = test_data.drug.apply(lambda x: 1 if x == \"D-penicil\" else 0)\n",
    "test_data.sex = test_data.sex.apply(lambda x: 1 if x == \"female\" else 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebcf426",
   "metadata": {},
   "source": [
    "## Prepare data for DL models\n",
    "\n",
    "Now we'll prepare the data for tensorflow models.\n",
    "\n",
    "We'll resample data to be equally sampled once a year.\n",
    "\n",
    "We'll only implement the simplest topology possible, using for every time $T_i$ the last 5 years of data and try to predict survival probability for the following 5 yeaers.\n",
    "\n",
    "\n",
    "The following code contains 1 *HUGE* bug, see if you can find it!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "\n",
    "continuous_columns = ['serBilir',\n",
    "    'serChol', 'albumin', 'alkaline', 'SGOT', 'platelets', 'prothrombin']\n",
    "\n",
    "categorical_columns = ['histologic', 'edema_No edema',\n",
    "    'edema_edema despite diuretics', 'edema_edema no diuretics',\n",
    "    'spiders_No', 'spiders_Yes', 'spiders_nan', 'hepatomegaly_No',\n",
    "    'hepatomegaly_Yes', 'hepatomegaly_nan', 'ascites_No', 'ascites_Yes',\n",
    "    'ascites_nan']\n",
    "\n",
    "fixed_columns = ['id', 'years', 'status', 'drug', 'age', 'sex']\n",
    "\n",
    "max_years = 6.0\n",
    "\n",
    "\n",
    "def resample_to_yearly_grid(my_data):\n",
    "\n",
    "    x_grid = np.arange(0, np.max(my_data.year), 1.0)\n",
    "\n",
    "    my_target = \"serBilir\"\n",
    "\n",
    "    def interpolate_continuous(my_target):\n",
    "        f = interpolate.interp1d(my_data.year, my_data[my_target])\n",
    "        return pd.DataFrame({my_target: f(x_grid)})\n",
    "\n",
    "    def interpolate_categorical(my_target):\n",
    "        f = interpolate.interp1d(my_data.year, my_data[my_target], kind=\"nearest\")\n",
    "        return pd.DataFrame({my_target: f(x_grid)})\n",
    "\n",
    "\n",
    "\n",
    "    resampled_continuous = pd.concat([interpolate_continuous(x) for x in continuous_columns], axis=1)\n",
    "    resampled_cat = pd.concat([interpolate_categorical(x) for x in categorical_columns], axis=1)\n",
    "    my_resampled = pd.concat([resampled_continuous, resampled_cat], axis=1)\n",
    "    for x in fixed_columns:\n",
    "        my_resampled[x] = my_data[x].iloc[0]\n",
    "    my_resampled[\"year\"] = x_grid\n",
    "    return my_resampled\n",
    "\n",
    "def get_x_y_sets(dataset):\n",
    "    def generate_data_for_id(my_id, dataset):\n",
    "        my_data = dataset[dataset.id == my_id]\n",
    "\n",
    "        my_data = resample_to_yearly_grid(my_data)\n",
    "\n",
    "        y = my_data.year\n",
    "\n",
    "\n",
    "        def generate_column_for_time(i):\n",
    "            def my_f(x):\n",
    "                if my_data.status.iloc[0] == \"dead\":\n",
    "                    return x + i > my_data.years.iloc[0] \n",
    "                else:\n",
    "                    return False\n",
    "            return my_data.year.apply(my_f)\n",
    "        def generate_mask_for_time(i):\n",
    "            def my_f(x):\n",
    "                if my_data.status.iloc[0] == \"dead\":\n",
    "                    return 0\n",
    "                elif x + i < my_data.years.iloc[0]:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 1\n",
    "            return my_data.year.apply(my_f)\n",
    "        y = pd.DataFrame([generate_column_for_time(i) for i in np.arange(1.0, max_years, 1.0)]).transpose()\n",
    "        y.columns = [f\"year_{int(x)}\" for x in np.arange(1.0, max_years, 1.0)]\n",
    "        y_mask = pd.DataFrame([generate_mask_for_time(i) for i in np.arange(1.0, max_years, 1.0)]).transpose()\n",
    "        y_mask.columns = [f\"mask_year_{int(x)}\" for x in np.arange(1.0, max_years, 1.0)]\n",
    "\n",
    "        y = pd.concat([y, y_mask], axis=1)\n",
    "\n",
    "        def generate_data_for_column(c):\n",
    "            return pd.DataFrame([my_data.loc[:, c].shift(i, axis=0) for i in range(max(5, len(my_data)))]).transpose().iloc[:, 0:5]\n",
    "\n",
    "        x = [generate_data_for_column(c) for c in longitudinal_columns] \n",
    "        x = pd.concat(x, axis=1)\n",
    "        x = pd.concat([x, my_data.loc[:, set(my_data.columns) - set(longitudinal_columns)]], axis=1)\n",
    "        x_mask = x.loc[:, continuous_columns].isna()\n",
    "        x = x.fillna(0)\n",
    "        x = pd.concat([x, x_mask], axis=1)\n",
    "        x[\"age\"] = x.age + x.year\n",
    "        x = x.drop(columns=[\"status\", \"year\"])\n",
    "        return x.astype(\"float\"), y.astype(\"float\")\n",
    "\n",
    "    data = [generate_data_for_id(x, dataset) for x in dataset.id.unique()]\n",
    "\n",
    "    x, y = zip(*data)\n",
    "    x = pd.concat(x)\n",
    "    x = x.reset_index(drop=True)\n",
    "    y = pd.concat(y)\n",
    "    y = y.reset_index(drop=True)\n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = get_x_y_sets(train_data)\n",
    "print(f\"Train: x has shape {x_train.shape}, y has shape {y_train.shape}\")\n",
    "\n",
    "x_test, y_test = get_x_y_sets(test_data)\n",
    "print(f\"Test: x has shape {x_test.shape}, y has shape {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac446315",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"removing columns with zero variance\")\n",
    "x_test = x_test.loc[:, (x_train != x_train.iloc[0]).any()] \n",
    "x_train = x_train.loc[:, (x_train != x_train.iloc[0]).any()] \n",
    "\n",
    "train_ids = x_train.id\n",
    "x_train = x_train.drop(columns=[\"id\"])\n",
    "test_ids = x_test.id\n",
    "x_test = x_test.drop(columns=[\"id\"])\n",
    "\n",
    "print(f\"Train: x has shape {x_train.shape}, y has shape {y_train.shape}\")\n",
    "print(f\"Test: x has shape {x_test.shape}, y has shape {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5847c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412fedd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def censored_binary_crossentropy(y_true, y_pred):\n",
    "    l = int(y_true.shape[1]/2)\n",
    "    y_true_mask = y_true[:,l:]\n",
    "    y_true_val = y_true[:,:l]\n",
    "    _epsilon = tf.keras.backend.epsilon()\n",
    "    y_pred_clipped = tf.clip_by_value(y_pred, _epsilon, 1 - _epsilon)\n",
    "    cross_entropy = - y_true_val * tf.math.log(y_pred_clipped) - (1.0 - y_true_val) * tf.math.log(1.0 - y_pred_clipped)\n",
    "    masked_cross_entropy = tf.where(y_true_mask == 1, tf.zeros_like(y_true_val), cross_entropy)\n",
    "    return tf.reduce_mean(masked_cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20366d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(x_train.shape[1]))\n",
    "x = layers.Dense(16, activation=\"relu\")(inputs)\n",
    "x = layers.Dense(int(y_train.shape[1]/2), activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, x)\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.0001,\n",
    "    patience=10,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    "    )\n",
    "]\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3, clipvalue=0.5),\n",
    "    loss = censored_binary_crossentropy,\n",
    ")\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train, \n",
    "    epochs=epochs, \n",
    "    callbacks=callbacks, \n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"model achieved {model.evaluate(x_test, y_test)} accuracy in test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6629e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b28be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_preds = pd.DataFrame(model.predict(x_test))\n",
    "my_preds = pd.concat([my_preds, y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "def calc_scores(i):\n",
    "    y_true = y_test.iloc[:, i]\n",
    "    y_mask = y_test.iloc[:, i + 5]\n",
    "    y_pred = my_preds.iloc[:, i]\n",
    "    F1 = sklearn.metrics.f1_score(y_true, np.round(y_pred), sample_weight = 1.0 - y_mask)\n",
    "    precision = sklearn.metrics.precision_score(y_true, np.round(y_pred), sample_weight = 1.0 - y_mask)\n",
    "    recall = sklearn.metrics.recall_score(y_true, np.round(y_pred), sample_weight = 1.0 - y_mask)\n",
    "    return precision, recall, F1, np.sum(y_mask)/len(y_mask)\n",
    "\n",
    "scores = pd.DataFrame([calc_scores(i) for i in range(5)])\n",
    "scores.columns = [\"precision\", \"recall\", \"F1\", \"perc masked\"]\n",
    "scores.index = [f\"at {i+1} year(s)\" for i in range(5)]\n",
    "\n",
    "plt.plot(scores.precision)\n",
    "plt.plot(scores.recall)\n",
    "plt.plot(scores.F1)\n",
    "\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d8ad8",
   "metadata": {},
   "source": [
    "If you didn't spot the bug you'll have very high F1 scores in test! \n",
    "\n",
    "HINT: If you haven't found the bug: study the train dataset... is there any column that is part of $x$ that is leaking the label?\n",
    "\n",
    "After fixing the bug, what F1 scores do we get now?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2463ae49",
   "metadata": {},
   "source": [
    "Now try changing the network topology, activation functions, number fo layers, etc. Split the train data in train and validation, using 5-fold CV. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab73435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0073551",
   "metadata": {},
   "source": [
    "Optional: add a second loss that approximates Concordance Index, or that predicts the values of longitudinal features at the next year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4928fcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbaaa18c4b7b6de00323c15a3402f45196af4d09cedda92c1bdd91862fbc8276"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
